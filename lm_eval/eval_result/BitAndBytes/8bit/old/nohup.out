mkdir -p results/all/x2bee/POLAR-14B-v0.2/
/home/ca2023/.asdf/installs/python/miniconda3-latest/envs/quantization/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Selected Tasks: ['kobest_hellaswag']

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]
Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:07,  1.55s/it]
Loading checkpoint shards:  33%|███▎      | 2/6 [00:03<00:06,  1.56s/it]
Loading checkpoint shards:  50%|█████     | 3/6 [00:04<00:04,  1.54s/it]
Loading checkpoint shards:  67%|██████▋   | 4/6 [00:06<00:03,  1.54s/it]
Loading checkpoint shards:  83%|████████▎ | 5/6 [00:07<00:01,  1.54s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:08<00:00,  1.42s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:08<00:00,  1.48s/it]
Task: kobest_hellaswag; number of docs: 500
Task: kobest_hellaswag; document 0; context prompt (starting on next line):
얼굴에 여드름이 난 여성이 거울 앞에 서있다. 여성이 거울 가까이에 얼굴을 대고 여드름을 바라본다.여성이 거울에서 물러나 면봉 두 개를 양 손으로 쥔다.
(end of prompt on previous line)
Requests: [Req_loglikelihood('얼굴에 여드름이 난 여성이 거울 앞에 서있다. 여성이 거울 가까이에 얼굴을 대고 여드름을 바라본다.여성이 거울에서 물러나 면봉 두 개를 양 손으로 쥔다.', ' 여성이 면봉으로 여드름 주변을 누른다.')[0]
, Req_loglikelihood('얼굴에 여드름이 난 여성이 거울 앞에 서있다. 여성이 거울 가까이에 얼굴을 대고 여드름을 바라본다.여성이 거울에서 물러나 면봉 두 개를 양 손으로 쥔다.', ' 여성이 여드름이 터져 나오는 피를 면봉으로 닦아낸다.')[0]
, Req_loglikelihood('얼굴에 여드름이 난 여성이 거울 앞에 서있다. 여성이 거울 가까이에 얼굴을 대고 여드름을 바라본다.여성이 거울에서 물러나 면봉 두 개를 양 손으로 쥔다.', ' 여드름이 터진다.')[0]
, Req_loglikelihood('얼굴에 여드름이 난 여성이 거울 앞에 서있다. 여성이 거울 가까이에 얼굴을 대고 여드름을 바라본다.여성이 거울에서 물러나 면봉 두 개를 양 손으로 쥔다.', ' 면봉 두 개를 여드름 가까이에 가져간다.')[0]
]
Running loglikelihood requests

  0%|          | 0/2000 [00:00<?, ?it/s]
  0%|          | 8/2000 [00:03<14:23,  2.31it/s]
  1%|          | 16/2000 [00:05<11:14,  2.94it/s]
  1%|          | 24/2000 [00:07<10:04,  3.27it/s]
  2%|▏         | 32/2000 [00:09<09:14,  3.55it/s]
  2%|▏         | 40/2000 [00:11<08:44,  3.73it/s]
  2%|▏         | 48/2000 [00:13<08:20,  3.90it/s]
  3%|▎         | 56/2000 [00:15<08:04,  4.01it/s]
  3%|▎         | 64/2000 [00:17<07:54,  4.08it/s]
  4%|▎         | 72/2000 [00:19<07:35,  4.23it/s]
  4%|▍         | 80/2000 [00:20<07:26,  4.30it/s]
  4%|▍         | 88/2000 [00:22<07:13,  4.41it/s]
  5%|▍         | 96/2000 [00:24<07:06,  4.47it/s]
  5%|▌         | 104/2000 [00:26<07:00,  4.51it/s]
  6%|▌         | 112/2000 [00:27<06:55,  4.54it/s]
  6%|▌         | 120/2000 [00:29<06:49,  4.59it/s]
  6%|▋         | 128/2000 [00:31<06:43,  4.64it/s]
  7%|▋         | 136/2000 [00:32<06:39,  4.67it/s]
  7%|▋         | 144/2000 [00:34<06:31,  4.74it/s]
  8%|▊         | 152/2000 [00:36<06:25,  4.80it/s]
  8%|▊         | 160/2000 [00:37<06:23,  4.80it/s]
  8%|▊         | 168/2000 [00:39<06:17,  4.85it/s]
  9%|▉         | 176/2000 [00:40<06:13,  4.88it/s]
  9%|▉         | 184/2000 [00:42<06:12,  4.87it/s]
 10%|▉         | 192/2000 [00:44<06:07,  4.92it/s]
 10%|█         | 200/2000 [00:45<06:01,  4.98it/s]
 10%|█         | 208/2000 [00:47<05:56,  5.03it/s]
 11%|█         | 216/2000 [00:48<05:51,  5.07it/s]
 11%|█         | 224/2000 [00:50<05:48,  5.09it/s]
 12%|█▏        | 232/2000 [00:52<05:48,  5.08it/s]
 12%|█▏        | 240/2000 [00:53<05:45,  5.10it/s]
 12%|█▏        | 248/2000 [00:55<05:42,  5.12it/s]
 13%|█▎        | 256/2000 [00:56<05:40,  5.13it/s]
 13%|█▎        | 264/2000 [00:58<05:37,  5.15it/s]
 14%|█▎        | 272/2000 [00:59<05:34,  5.16it/s]
 14%|█▍        | 280/2000 [01:01<05:33,  5.15it/s]
 14%|█▍        | 288/2000 [01:02<05:29,  5.19it/s]
 15%|█▍        | 296/2000 [01:04<05:25,  5.23it/s]
 15%|█▌        | 304/2000 [01:05<05:22,  5.27it/s]
 16%|█▌        | 312/2000 [01:07<05:19,  5.29it/s]
 16%|█▌        | 320/2000 [01:08<05:19,  5.26it/s]
 16%|█▋        | 328/2000 [01:10<05:20,  5.22it/s]
 17%|█▋        | 336/2000 [01:11<05:16,  5.26it/s]
 17%|█▋        | 344/2000 [01:13<05:13,  5.29it/s]
 18%|█▊        | 352/2000 [01:14<05:10,  5.30it/s]
 18%|█▊        | 360/2000 [01:16<05:08,  5.32it/s]
 18%|█▊        | 368/2000 [01:17<05:06,  5.33it/s]
 19%|█▉        | 376/2000 [01:19<05:04,  5.33it/s]
 19%|█▉        | 384/2000 [01:20<05:03,  5.32it/s]
 20%|█▉        | 392/2000 [01:22<05:01,  5.34it/s]
 20%|██        | 400/2000 [01:23<04:58,  5.36it/s]
 20%|██        | 408/2000 [01:25<04:58,  5.34it/s]
 21%|██        | 416/2000 [01:26<04:53,  5.40it/s]
 21%|██        | 424/2000 [01:28<04:48,  5.46it/s]
 22%|██▏       | 432/2000 [01:29<04:44,  5.51it/s]
 22%|██▏       | 440/2000 [01:31<04:41,  5.53it/s]
 22%|██▏       | 448/2000 [01:32<04:39,  5.54it/s]
 23%|██▎       | 456/2000 [01:33<04:38,  5.54it/s]
 23%|██▎       | 464/2000 [01:35<04:37,  5.53it/s]
 24%|██▎       | 472/2000 [01:36<04:35,  5.55it/s]
 24%|██▍       | 480/2000 [01:38<04:33,  5.55it/s]
 24%|██▍       | 488/2000 [01:39<04:32,  5.54it/s]
 25%|██▍       | 496/2000 [01:41<04:28,  5.60it/s]
 25%|██▌       | 504/2000 [01:42<04:25,  5.64it/s]
 26%|██▌       | 512/2000 [01:43<04:24,  5.62it/s]
 26%|██▌       | 520/2000 [01:45<04:23,  5.62it/s]
 26%|██▋       | 528/2000 [01:46<04:21,  5.63it/s]
 27%|██▋       | 536/2000 [01:48<04:20,  5.62it/s]
 27%|██▋       | 544/2000 [01:49<04:16,  5.67it/s]
 28%|██▊       | 552/2000 [01:51<04:15,  5.67it/s]
 28%|██▊       | 560/2000 [01:52<04:14,  5.66it/s]
 28%|██▊       | 568/2000 [01:53<04:12,  5.67it/s]
 29%|██▉       | 576/2000 [01:55<04:11,  5.67it/s]
 29%|██▉       | 584/2000 [01:56<04:07,  5.72it/s]
 30%|██▉       | 592/2000 [01:58<04:04,  5.76it/s]
 30%|███       | 600/2000 [01:59<04:02,  5.78it/s]
 30%|███       | 608/2000 [02:00<03:59,  5.80it/s]
 31%|███       | 616/2000 [02:02<03:59,  5.79it/s]
 31%|███       | 624/2000 [02:03<03:57,  5.79it/s]
 32%|███▏      | 632/2000 [02:04<03:55,  5.80it/s]
 32%|███▏      | 640/2000 [02:06<03:52,  5.84it/s]
 32%|███▏      | 648/2000 [02:07<03:50,  5.86it/s]
 33%|███▎      | 656/2000 [02:08<03:48,  5.87it/s]
 33%|███▎      | 664/2000 [02:10<03:45,  5.93it/s]
 34%|███▎      | 672/2000 [02:11<03:44,  5.92it/s]
 34%|███▍      | 680/2000 [02:12<03:41,  5.96it/s]
 34%|███▍      | 688/2000 [02:14<03:40,  5.95it/s]
 35%|███▍      | 696/2000 [02:15<03:38,  5.97it/s]
 35%|███▌      | 704/2000 [02:16<03:37,  5.95it/s]
 36%|███▌      | 712/2000 [02:18<03:36,  5.94it/s]
 36%|███▌      | 720/2000 [02:19<03:35,  5.94it/s]
 36%|███▋      | 728/2000 [02:21<03:33,  5.95it/s]
 37%|███▋      | 736/2000 [02:22<03:33,  5.91it/s]
 37%|███▋      | 744/2000 [02:23<03:31,  5.94it/s]
 38%|███▊      | 752/2000 [02:25<03:30,  5.92it/s]
 38%|███▊      | 760/2000 [02:26<03:28,  5.95it/s]
 38%|███▊      | 768/2000 [02:27<03:26,  5.97it/s]
 39%|███▉      | 776/2000 [02:29<03:24,  5.97it/s]
 39%|███▉      | 784/2000 [02:30<03:22,  6.02it/s]
 40%|███▉      | 792/2000 [02:31<03:20,  6.04it/s]
 40%|████      | 800/2000 [02:33<03:18,  6.04it/s]
 40%|████      | 808/2000 [02:34<03:17,  6.03it/s]
 41%|████      | 816/2000 [02:35<03:15,  6.06it/s]
 41%|████      | 824/2000 [02:36<03:13,  6.08it/s]
 42%|████▏     | 832/2000 [02:38<03:11,  6.09it/s]
 42%|████▏     | 840/2000 [02:39<03:09,  6.13it/s]
 42%|████▏     | 848/2000 [02:40<03:08,  6.12it/s]
 43%|████▎     | 856/2000 [02:42<03:06,  6.14it/s]
 43%|████▎     | 864/2000 [02:43<03:04,  6.14it/s]
 44%|████▎     | 872/2000 [02:44<03:02,  6.17it/s]
 44%|████▍     | 880/2000 [02:46<03:00,  6.20it/s]
 44%|████▍     | 888/2000 [02:47<02:59,  6.20it/s]
 45%|████▍     | 896/2000 [02:48<02:58,  6.18it/s]
 45%|████▌     | 904/2000 [02:49<02:56,  6.21it/s]
 46%|████▌     | 912/2000 [02:51<02:55,  6.21it/s]
 46%|████▌     | 920/2000 [02:52<02:54,  6.20it/s]
 46%|████▋     | 928/2000 [02:53<02:52,  6.22it/s]
 47%|████▋     | 936/2000 [02:55<02:50,  6.23it/s]
 47%|████▋     | 944/2000 [02:56<02:49,  6.24it/s]
 48%|████▊     | 952/2000 [02:57<02:48,  6.21it/s]
 48%|████▊     | 960/2000 [02:58<02:46,  6.25it/s]
 48%|████▊     | 968/2000 [03:00<02:45,  6.25it/s]
 49%|████▉     | 976/2000 [03:01<02:43,  6.28it/s]
 49%|████▉     | 984/2000 [03:02<02:41,  6.29it/s]
 50%|████▉     | 992/2000 [03:03<02:40,  6.30it/s]
 50%|█████     | 1000/2000 [03:05<02:38,  6.32it/s]
 50%|█████     | 1008/2000 [03:06<02:37,  6.32it/s]
 51%|█████     | 1016/2000 [03:07<02:35,  6.34it/s]
 51%|█████     | 1024/2000 [03:08<02:33,  6.35it/s]
 52%|█████▏    | 1032/2000 [03:10<02:32,  6.36it/s]
 52%|█████▏    | 1040/2000 [03:11<02:31,  6.36it/s]
 52%|█████▏    | 1048/2000 [03:12<02:29,  6.35it/s]
 53%|█████▎    | 1056/2000 [03:14<02:28,  6.35it/s]
 53%|█████▎    | 1064/2000 [03:15<02:27,  6.36it/s]
 54%|█████▎    | 1072/2000 [03:16<02:25,  6.36it/s]
 54%|█████▍    | 1080/2000 [03:17<02:24,  6.36it/s]
 54%|█████▍    | 1088/2000 [03:19<02:22,  6.40it/s]
 55%|█████▍    | 1096/2000 [03:20<02:21,  6.39it/s]
 55%|█████▌    | 1104/2000 [03:21<02:20,  6.37it/s]
 56%|█████▌    | 1112/2000 [03:22<02:19,  6.37it/s]
 56%|█████▌    | 1120/2000 [03:24<02:18,  6.35it/s]
 56%|█████▋    | 1128/2000 [03:25<02:16,  6.41it/s]
 57%|█████▋    | 1136/2000 [03:26<02:14,  6.43it/s]
 57%|█████▋    | 1144/2000 [03:27<02:12,  6.45it/s]
 58%|█████▊    | 1152/2000 [03:28<02:10,  6.48it/s]
 58%|█████▊    | 1160/2000 [03:30<02:09,  6.49it/s]
 58%|█████▊    | 1168/2000 [03:31<02:08,  6.50it/s]
 59%|█████▉    | 1176/2000 [03:32<02:07,  6.47it/s]
 59%|█████▉    | 1184/2000 [03:33<02:05,  6.51it/s]
 60%|█████▉    | 1192/2000 [03:35<02:03,  6.54it/s]
 60%|██████    | 1200/2000 [03:36<02:01,  6.56it/s]
 60%|██████    | 1208/2000 [03:37<02:00,  6.59it/s]
 61%|██████    | 1216/2000 [03:38<01:58,  6.59it/s]
 61%|██████    | 1224/2000 [03:39<01:57,  6.60it/s]
 62%|██████▏   | 1232/2000 [03:41<01:56,  6.60it/s]
 62%|██████▏   | 1240/2000 [03:42<01:54,  6.62it/s]
 62%|██████▏   | 1248/2000 [03:43<01:53,  6.62it/s]
 63%|██████▎   | 1256/2000 [03:44<01:51,  6.64it/s]
 63%|██████▎   | 1264/2000 [03:45<01:50,  6.67it/s]
 64%|██████▎   | 1272/2000 [03:47<01:49,  6.63it/s]
 64%|██████▍   | 1280/2000 [03:48<01:48,  6.62it/s]
 64%|██████▍   | 1288/2000 [03:49<01:47,  6.61it/s]
 65%|██████▍   | 1296/2000 [03:50<01:46,  6.61it/s]
 65%|██████▌   | 1304/2000 [03:52<01:45,  6.58it/s]
 66%|██████▌   | 1312/2000 [03:53<01:44,  6.60it/s]
 66%|██████▌   | 1320/2000 [03:54<01:43,  6.60it/s]
 66%|██████▋   | 1328/2000 [03:55<01:40,  6.66it/s]
 67%|██████▋   | 1336/2000 [03:56<01:39,  6.65it/s]
 67%|██████▋   | 1344/2000 [03:58<01:38,  6.68it/s]
 68%|██████▊   | 1352/2000 [03:59<01:37,  6.68it/s]
 68%|██████▊   | 1360/2000 [04:00<01:35,  6.68it/s]
 68%|██████▊   | 1368/2000 [04:01<01:34,  6.69it/s]
 69%|██████▉   | 1376/2000 [04:02<01:33,  6.64it/s]
 69%|██████▉   | 1384/2000 [04:04<01:32,  6.66it/s]
 70%|██████▉   | 1392/2000 [04:05<01:31,  6.67it/s]
 70%|███████   | 1400/2000 [04:06<01:29,  6.72it/s]
 70%|███████   | 1408/2000 [04:07<01:28,  6.69it/s]
 71%|███████   | 1416/2000 [04:08<01:27,  6.71it/s]
 71%|███████   | 1424/2000 [04:09<01:25,  6.72it/s]
 72%|███████▏  | 1432/2000 [04:11<01:24,  6.74it/s]
 72%|███████▏  | 1440/2000 [04:12<01:22,  6.78it/s]
 72%|███████▏  | 1448/2000 [04:13<01:21,  6.79it/s]
 73%|███████▎  | 1456/2000 [04:14<01:20,  6.77it/s]
 73%|███████▎  | 1464/2000 [04:15<01:19,  6.76it/s]
 74%|███████▎  | 1472/2000 [04:17<01:18,  6.76it/s]
 74%|███████▍  | 1480/2000 [04:18<01:16,  6.76it/s]
 74%|███████▍  | 1488/2000 [04:19<01:15,  6.77it/s]
 75%|███████▍  | 1496/2000 [04:20<01:14,  6.76it/s]
 75%|███████▌  | 1504/2000 [04:21<01:13,  6.74it/s]
 76%|███████▌  | 1512/2000 [04:22<01:11,  6.83it/s]
 76%|███████▌  | 1520/2000 [04:24<01:09,  6.89it/s]
 76%|███████▋  | 1528/2000 [04:25<01:08,  6.92it/s]
 77%|███████▋  | 1536/2000 [04:26<01:06,  6.98it/s]
 77%|███████▋  | 1544/2000 [04:27<01:05,  7.02it/s]
 78%|███████▊  | 1552/2000 [04:28<01:03,  7.03it/s]
 78%|███████▊  | 1560/2000 [04:29<01:02,  7.05it/s]
 78%|███████▊  | 1568/2000 [04:30<01:01,  7.06it/s]
 79%|███████▉  | 1576/2000 [04:31<00:59,  7.08it/s]
 79%|███████▉  | 1584/2000 [04:33<00:58,  7.10it/s]
 80%|███████▉  | 1592/2000 [04:34<00:57,  7.11it/s]
 80%|████████  | 1600/2000 [04:35<00:56,  7.10it/s]
 80%|████████  | 1608/2000 [04:36<00:55,  7.09it/s]
 81%|████████  | 1616/2000 [04:37<00:53,  7.13it/s]
 81%|████████  | 1624/2000 [04:38<00:52,  7.21it/s]
 82%|████████▏ | 1632/2000 [04:39<00:51,  7.18it/s]
 82%|████████▏ | 1640/2000 [04:40<00:49,  7.20it/s]
 82%|████████▏ | 1648/2000 [04:41<00:48,  7.22it/s]
 83%|████████▎ | 1656/2000 [04:43<00:47,  7.22it/s]
 83%|████████▎ | 1664/2000 [04:44<00:46,  7.20it/s]
 84%|████████▎ | 1672/2000 [04:45<00:45,  7.19it/s]
 84%|████████▍ | 1680/2000 [04:46<00:44,  7.19it/s]
 84%|████████▍ | 1688/2000 [04:47<00:43,  7.13it/s]
 85%|████████▍ | 1696/2000 [04:48<00:42,  7.15it/s]
 85%|████████▌ | 1704/2000 [04:49<00:41,  7.20it/s]
 86%|████████▌ | 1712/2000 [04:50<00:39,  7.21it/s]
 86%|████████▌ | 1720/2000 [04:51<00:38,  7.21it/s]
 86%|████████▋ | 1728/2000 [04:53<00:37,  7.16it/s]
 87%|████████▋ | 1736/2000 [04:54<00:37,  7.12it/s]
 87%|████████▋ | 1744/2000 [04:55<00:35,  7.13it/s]
 88%|████████▊ | 1752/2000 [04:56<00:34,  7.15it/s]
 88%|████████▊ | 1760/2000 [04:57<00:33,  7.18it/s]
 88%|████████▊ | 1768/2000 [04:58<00:32,  7.20it/s]
 89%|████████▉ | 1776/2000 [04:59<00:31,  7.20it/s]
 89%|████████▉ | 1784/2000 [05:00<00:29,  7.22it/s]
 90%|████████▉ | 1792/2000 [05:02<00:28,  7.22it/s]
 90%|█████████ | 1800/2000 [05:03<00:27,  7.26it/s]
 90%|█████████ | 1808/2000 [05:04<00:26,  7.26it/s]
 91%|█████████ | 1816/2000 [05:05<00:25,  7.29it/s]
 91%|█████████ | 1824/2000 [05:06<00:24,  7.31it/s]
 92%|█████████▏| 1832/2000 [05:07<00:22,  7.31it/s]
 92%|█████████▏| 1840/2000 [05:08<00:21,  7.32it/s]
 92%|█████████▏| 1848/2000 [05:09<00:20,  7.35it/s]
 93%|█████████▎| 1856/2000 [05:10<00:19,  7.41it/s]
 93%|█████████▎| 1864/2000 [05:11<00:18,  7.41it/s]
 94%|█████████▎| 1872/2000 [05:12<00:17,  7.42it/s]
 94%|█████████▍| 1880/2000 [05:13<00:16,  7.41it/s]
 94%|█████████▍| 1888/2000 [05:15<00:15,  7.41it/s]
 95%|█████████▍| 1896/2000 [05:16<00:13,  7.43it/s]
 95%|█████████▌| 1904/2000 [05:17<00:12,  7.42it/s]
 96%|█████████▌| 1912/2000 [05:18<00:11,  7.43it/s]
 96%|█████████▌| 1920/2000 [05:19<00:10,  7.45it/s]
 96%|█████████▋| 1928/2000 [05:20<00:09,  7.49it/s]
 97%|█████████▋| 1936/2000 [05:21<00:08,  7.50it/s]
 97%|█████████▋| 1944/2000 [05:22<00:07,  7.52it/s]
 98%|█████████▊| 1952/2000 [05:23<00:06,  7.54it/s]
 98%|█████████▊| 1960/2000 [05:24<00:05,  7.54it/s]
 98%|█████████▊| 1968/2000 [05:25<00:04,  7.56it/s]
 99%|█████████▉| 1976/2000 [05:26<00:03,  7.59it/s]
 99%|█████████▉| 1984/2000 [05:27<00:02,  7.61it/s]
100%|█████████▉| 1992/2000 [05:28<00:01,  7.71it/s]
100%|██████████| 2000/2000 [05:29<00:00,  7.83it/s]
100%|██████████| 2000/2000 [05:29<00:00,  6.07it/s]
bootstrapping for stddev: macro_f1_score

  0%|          | 0/100 [00:00<?, ?it/s]
  1%|          | 1/100 [00:03<05:38,  3.42s/it]
 42%|████▏     | 42/100 [00:03<00:03, 16.33it/s]
 66%|██████▌   | 66/100 [00:03<00:01, 27.19it/s]
 81%|████████  | 81/100 [00:05<00:01, 15.96it/s]
 91%|█████████ | 91/100 [00:06<00:00, 17.23it/s]
100%|██████████| 100/100 [00:06<00:00, 16.44it/s]

[INFO] Model parameter types:
 - model.embed_tokens.weight: torch.float16
 - model.layers.0.self_attn.q_proj.weight: torch.int8
 - model.layers.0.self_attn.k_proj.weight: torch.int8
 - model.layers.0.self_attn.v_proj.weight: torch.int8
 - model.layers.0.self_attn.o_proj.weight: torch.int8
 - model.layers.0.mlp.gate_proj.weight: torch.int8
 - model.layers.0.mlp.up_proj.weight: torch.int8
 - model.layers.0.mlp.down_proj.weight: torch.int8
 - model.layers.0.input_layernorm.weight: torch.float16
 - model.layers.0.post_attention_layernorm.weight: torch.float16
 - model.layers.1.self_attn.q_proj.weight: torch.int8
 - model.layers.1.self_attn.k_proj.weight: torch.int8
 - model.layers.1.self_attn.v_proj.weight: torch.int8
 - model.layers.1.self_attn.o_proj.weight: torch.int8
 - model.layers.1.mlp.gate_proj.weight: torch.int8
 - model.layers.1.mlp.up_proj.weight: torch.int8
 - model.layers.1.mlp.down_proj.weight: torch.int8
 - model.layers.1.input_layernorm.weight: torch.float16
 - model.layers.1.post_attention_layernorm.weight: torch.float16
 - model.layers.2.self_attn.q_proj.weight: torch.int8
 - model.layers.2.self_attn.k_proj.weight: torch.int8
 - model.layers.2.self_attn.v_proj.weight: torch.int8
 - model.layers.2.self_attn.o_proj.weight: torch.int8
 - model.layers.2.mlp.gate_proj.weight: torch.int8
 - model.layers.2.mlp.up_proj.weight: torch.int8
 - model.layers.2.mlp.down_proj.weight: torch.int8
 - model.layers.2.input_layernorm.weight: torch.float16
 - model.layers.2.post_attention_layernorm.weight: torch.float16
 - model.layers.3.self_attn.q_proj.weight: torch.int8
 - model.layers.3.self_attn.k_proj.weight: torch.int8
 - model.layers.3.self_attn.v_proj.weight: torch.int8
 - model.layers.3.self_attn.o_proj.weight: torch.int8
 - model.layers.3.mlp.gate_proj.weight: torch.int8
 - model.layers.3.mlp.up_proj.weight: torch.int8
 - model.layers.3.mlp.down_proj.weight: torch.int8
 - model.layers.3.input_layernorm.weight: torch.float16
 - model.layers.3.post_attention_layernorm.weight: torch.float16
 - model.layers.4.self_attn.q_proj.weight: torch.int8
 - model.layers.4.self_attn.k_proj.weight: torch.int8
 - model.layers.4.self_attn.v_proj.weight: torch.int8
 - model.layers.4.self_attn.o_proj.weight: torch.int8
 - model.layers.4.mlp.gate_proj.weight: torch.int8
 - model.layers.4.mlp.up_proj.weight: torch.int8
 - model.layers.4.mlp.down_proj.weight: torch.int8
 - model.layers.4.input_layernorm.weight: torch.float16
 - model.layers.4.post_attention_layernorm.weight: torch.float16
 - model.layers.5.self_attn.q_proj.weight: torch.int8
 - model.layers.5.self_attn.k_proj.weight: torch.int8
 - model.layers.5.self_attn.v_proj.weight: torch.int8
 - model.layers.5.self_attn.o_proj.weight: torch.int8
 - model.layers.5.mlp.gate_proj.weight: torch.int8
 - model.layers.5.mlp.up_proj.weight: torch.int8
 - model.layers.5.mlp.down_proj.weight: torch.int8
 - model.layers.5.input_layernorm.weight: torch.float16
 - model.layers.5.post_attention_layernorm.weight: torch.float16
 - model.layers.6.self_attn.q_proj.weight: torch.int8
 - model.layers.6.self_attn.k_proj.weight: torch.int8
 - model.layers.6.self_attn.v_proj.weight: torch.int8
 - model.layers.6.self_attn.o_proj.weight: torch.int8
 - model.layers.6.mlp.gate_proj.weight: torch.int8
 - model.layers.6.mlp.up_proj.weight: torch.int8
 - model.layers.6.mlp.down_proj.weight: torch.int8
 - model.layers.6.input_layernorm.weight: torch.float16
 - model.layers.6.post_attention_layernorm.weight: torch.float16
 - model.layers.7.self_attn.q_proj.weight: torch.int8
 - model.layers.7.self_attn.k_proj.weight: torch.int8
 - model.layers.7.self_attn.v_proj.weight: torch.int8
 - model.layers.7.self_attn.o_proj.weight: torch.int8
 - model.layers.7.mlp.gate_proj.weight: torch.int8
 - model.layers.7.mlp.up_proj.weight: torch.int8
 - model.layers.7.mlp.down_proj.weight: torch.int8
 - model.layers.7.input_layernorm.weight: torch.float16
 - model.layers.7.post_attention_layernorm.weight: torch.float16
 - model.layers.8.self_attn.q_proj.weight: torch.int8
 - model.layers.8.self_attn.k_proj.weight: torch.int8
 - model.layers.8.self_attn.v_proj.weight: torch.int8
 - model.layers.8.self_attn.o_proj.weight: torch.int8
 - model.layers.8.mlp.gate_proj.weight: torch.int8
 - model.layers.8.mlp.up_proj.weight: torch.int8
 - model.layers.8.mlp.down_proj.weight: torch.int8
 - model.layers.8.input_layernorm.weight: torch.float16
 - model.layers.8.post_attention_layernorm.weight: torch.float16
 - model.layers.9.self_attn.q_proj.weight: torch.int8
 - model.layers.9.self_attn.k_proj.weight: torch.int8
 - model.layers.9.self_attn.v_proj.weight: torch.int8
 - model.layers.9.self_attn.o_proj.weight: torch.int8
 - model.layers.9.mlp.gate_proj.weight: torch.int8
 - model.layers.9.mlp.up_proj.weight: torch.int8
 - model.layers.9.mlp.down_proj.weight: torch.int8
 - model.layers.9.input_layernorm.weight: torch.float16
 - model.layers.9.post_attention_layernorm.weight: torch.float16
 - model.layers.10.self_attn.q_proj.weight: torch.int8
 - model.layers.10.self_attn.k_proj.weight: torch.int8
 - model.layers.10.self_attn.v_proj.weight: torch.int8
 - model.layers.10.self_attn.o_proj.weight: torch.int8
 - model.layers.10.mlp.gate_proj.weight: torch.int8
 - model.layers.10.mlp.up_proj.weight: torch.int8
 - model.layers.10.mlp.down_proj.weight: torch.int8
 - model.layers.10.input_layernorm.weight: torch.float16
 - model.layers.10.post_attention_layernorm.weight: torch.float16
 - model.layers.11.self_attn.q_proj.weight: torch.int8
 - model.layers.11.self_attn.k_proj.weight: torch.int8
 - model.layers.11.self_attn.v_proj.weight: torch.int8
 - model.layers.11.self_attn.o_proj.weight: torch.int8
 - model.layers.11.mlp.gate_proj.weight: torch.int8
 - model.layers.11.mlp.up_proj.weight: torch.int8
 - model.layers.11.mlp.down_proj.weight: torch.int8
 - model.layers.11.input_layernorm.weight: torch.float16
 - model.layers.11.post_attention_layernorm.weight: torch.float16
 - model.layers.12.self_attn.q_proj.weight: torch.int8
 - model.layers.12.self_attn.k_proj.weight: torch.int8
 - model.layers.12.self_attn.v_proj.weight: torch.int8
 - model.layers.12.self_attn.o_proj.weight: torch.int8
 - model.layers.12.mlp.gate_proj.weight: torch.int8
 - model.layers.12.mlp.up_proj.weight: torch.int8
 - model.layers.12.mlp.down_proj.weight: torch.int8
 - model.layers.12.input_layernorm.weight: torch.float16
 - model.layers.12.post_attention_layernorm.weight: torch.float16
 - model.layers.13.self_attn.q_proj.weight: torch.int8
 - model.layers.13.self_attn.k_proj.weight: torch.int8
 - model.layers.13.self_attn.v_proj.weight: torch.int8
 - model.layers.13.self_attn.o_proj.weight: torch.int8
 - model.layers.13.mlp.gate_proj.weight: torch.int8
 - model.layers.13.mlp.up_proj.weight: torch.int8
 - model.layers.13.mlp.down_proj.weight: torch.int8
 - model.layers.13.input_layernorm.weight: torch.float16
 - model.layers.13.post_attention_layernorm.weight: torch.float16
 - model.layers.14.self_attn.q_proj.weight: torch.int8
 - model.layers.14.self_attn.k_proj.weight: torch.int8
 - model.layers.14.self_attn.v_proj.weight: torch.int8
 - model.layers.14.self_attn.o_proj.weight: torch.int8
 - model.layers.14.mlp.gate_proj.weight: torch.int8
 - model.layers.14.mlp.up_proj.weight: torch.int8
 - model.layers.14.mlp.down_proj.weight: torch.int8
 - model.layers.14.input_layernorm.weight: torch.float16
 - model.layers.14.post_attention_layernorm.weight: torch.float16
 - model.layers.15.self_attn.q_proj.weight: torch.int8
 - model.layers.15.self_attn.k_proj.weight: torch.int8
 - model.layers.15.self_attn.v_proj.weight: torch.int8
 - model.layers.15.self_attn.o_proj.weight: torch.int8
 - model.layers.15.mlp.gate_proj.weight: torch.int8
 - model.layers.15.mlp.up_proj.weight: torch.int8
 - model.layers.15.mlp.down_proj.weight: torch.int8
 - model.layers.15.input_layernorm.weight: torch.float16
 - model.layers.15.post_attention_layernorm.weight: torch.float16
 - model.layers.16.self_attn.q_proj.weight: torch.int8
 - model.layers.16.self_attn.k_proj.weight: torch.int8
 - model.layers.16.self_attn.v_proj.weight: torch.int8
 - model.layers.16.self_attn.o_proj.weight: torch.int8
 - model.layers.16.mlp.gate_proj.weight: torch.int8
 - model.layers.16.mlp.up_proj.weight: torch.int8
 - model.layers.16.mlp.down_proj.weight: torch.int8
 - model.layers.16.input_layernorm.weight: torch.float16
 - model.layers.16.post_attention_layernorm.weight: torch.float16
 - model.layers.17.self_attn.q_proj.weight: torch.int8
 - model.layers.17.self_attn.k_proj.weight: torch.int8
 - model.layers.17.self_attn.v_proj.weight: torch.int8
 - model.layers.17.self_attn.o_proj.weight: torch.int8
 - model.layers.17.mlp.gate_proj.weight: torch.int8
 - model.layers.17.mlp.up_proj.weight: torch.int8
 - model.layers.17.mlp.down_proj.weight: torch.int8
 - model.layers.17.input_layernorm.weight: torch.float16
 - model.layers.17.post_attention_layernorm.weight: torch.float16
 - model.layers.18.self_attn.q_proj.weight: torch.int8
 - model.layers.18.self_attn.k_proj.weight: torch.int8
 - model.layers.18.self_attn.v_proj.weight: torch.int8
 - model.layers.18.self_attn.o_proj.weight: torch.int8
 - model.layers.18.mlp.gate_proj.weight: torch.int8
 - model.layers.18.mlp.up_proj.weight: torch.int8
 - model.layers.18.mlp.down_proj.weight: torch.int8
 - model.layers.18.input_layernorm.weight: torch.float16
 - model.layers.18.post_attention_layernorm.weight: torch.float16
 - model.layers.19.self_attn.q_proj.weight: torch.int8
 - model.layers.19.self_attn.k_proj.weight: torch.int8
 - model.layers.19.self_attn.v_proj.weight: torch.int8
 - model.layers.19.self_attn.o_proj.weight: torch.int8
 - model.layers.19.mlp.gate_proj.weight: torch.int8
 - model.layers.19.mlp.up_proj.weight: torch.int8
 - model.layers.19.mlp.down_proj.weight: torch.int8
 - model.layers.19.input_layernorm.weight: torch.float16
 - model.layers.19.post_attention_layernorm.weight: torch.float16
 - model.layers.20.self_attn.q_proj.weight: torch.int8
 - model.layers.20.self_attn.k_proj.weight: torch.int8
 - model.layers.20.self_attn.v_proj.weight: torch.int8
 - model.layers.20.self_attn.o_proj.weight: torch.int8
 - model.layers.20.mlp.gate_proj.weight: torch.int8
 - model.layers.20.mlp.up_proj.weight: torch.int8
 - model.layers.20.mlp.down_proj.weight: torch.int8
 - model.layers.20.input_layernorm.weight: torch.float16
 - model.layers.20.post_attention_layernorm.weight: torch.float16
 - model.layers.21.self_attn.q_proj.weight: torch.int8
 - model.layers.21.self_attn.k_proj.weight: torch.int8
 - model.layers.21.self_attn.v_proj.weight: torch.int8
 - model.layers.21.self_attn.o_proj.weight: torch.int8
 - model.layers.21.mlp.gate_proj.weight: torch.int8
 - model.layers.21.mlp.up_proj.weight: torch.int8
 - model.layers.21.mlp.down_proj.weight: torch.int8
 - model.layers.21.input_layernorm.weight: torch.float16
 - model.layers.21.post_attention_layernorm.weight: torch.float16
 - model.layers.22.self_attn.q_proj.weight: torch.int8
 - model.layers.22.self_attn.k_proj.weight: torch.int8
 - model.layers.22.self_attn.v_proj.weight: torch.int8
 - model.layers.22.self_attn.o_proj.weight: torch.int8
 - model.layers.22.mlp.gate_proj.weight: torch.int8
 - model.layers.22.mlp.up_proj.weight: torch.int8
 - model.layers.22.mlp.down_proj.weight: torch.int8
 - model.layers.22.input_layernorm.weight: torch.float16
 - model.layers.22.post_attention_layernorm.weight: torch.float16
 - model.layers.23.self_attn.q_proj.weight: torch.int8
 - model.layers.23.self_attn.k_proj.weight: torch.int8
 - model.layers.23.self_attn.v_proj.weight: torch.int8
 - model.layers.23.self_attn.o_proj.weight: torch.int8
 - model.layers.23.mlp.gate_proj.weight: torch.int8
 - model.layers.23.mlp.up_proj.weight: torch.int8
 - model.layers.23.mlp.down_proj.weight: torch.int8
 - model.layers.23.input_layernorm.weight: torch.float16
 - model.layers.23.post_attention_layernorm.weight: torch.float16
 - model.layers.24.self_attn.q_proj.weight: torch.int8
 - model.layers.24.self_attn.k_proj.weight: torch.int8
 - model.layers.24.self_attn.v_proj.weight: torch.int8
 - model.layers.24.self_attn.o_proj.weight: torch.int8
 - model.layers.24.mlp.gate_proj.weight: torch.int8
 - model.layers.24.mlp.up_proj.weight: torch.int8
 - model.layers.24.mlp.down_proj.weight: torch.int8
 - model.layers.24.input_layernorm.weight: torch.float16
 - model.layers.24.post_attention_layernorm.weight: torch.float16
 - model.layers.25.self_attn.q_proj.weight: torch.int8
 - model.layers.25.self_attn.k_proj.weight: torch.int8
 - model.layers.25.self_attn.v_proj.weight: torch.int8
 - model.layers.25.self_attn.o_proj.weight: torch.int8
 - model.layers.25.mlp.gate_proj.weight: torch.int8
 - model.layers.25.mlp.up_proj.weight: torch.int8
 - model.layers.25.mlp.down_proj.weight: torch.int8
 - model.layers.25.input_layernorm.weight: torch.float16
 - model.layers.25.post_attention_layernorm.weight: torch.float16
 - model.layers.26.self_attn.q_proj.weight: torch.int8
 - model.layers.26.self_attn.k_proj.weight: torch.int8
 - model.layers.26.self_attn.v_proj.weight: torch.int8
 - model.layers.26.self_attn.o_proj.weight: torch.int8
 - model.layers.26.mlp.gate_proj.weight: torch.int8
 - model.layers.26.mlp.up_proj.weight: torch.int8
 - model.layers.26.mlp.down_proj.weight: torch.int8
 - model.layers.26.input_layernorm.weight: torch.float16
 - model.layers.26.post_attention_layernorm.weight: torch.float16
 - model.layers.27.self_attn.q_proj.weight: torch.int8
 - model.layers.27.self_attn.k_proj.weight: torch.int8
 - model.layers.27.self_attn.v_proj.weight: torch.int8
 - model.layers.27.self_attn.o_proj.weight: torch.int8
 - model.layers.27.mlp.gate_proj.weight: torch.int8
 - model.layers.27.mlp.up_proj.weight: torch.int8
 - model.layers.27.mlp.down_proj.weight: torch.int8
 - model.layers.27.input_layernorm.weight: torch.float16
 - model.layers.27.post_attention_layernorm.weight: torch.float16
 - model.layers.28.self_attn.q_proj.weight: torch.int8
 - model.layers.28.self_attn.k_proj.weight: torch.int8
 - model.layers.28.self_attn.v_proj.weight: torch.int8
 - model.layers.28.self_attn.o_proj.weight: torch.int8
 - model.layers.28.mlp.gate_proj.weight: torch.int8
 - model.layers.28.mlp.up_proj.weight: torch.int8
 - model.layers.28.mlp.down_proj.weight: torch.int8
 - model.layers.28.input_layernorm.weight: torch.float16
 - model.layers.28.post_attention_layernorm.weight: torch.float16
 - model.layers.29.self_attn.q_proj.weight: torch.int8
 - model.layers.29.self_attn.k_proj.weight: torch.int8
 - model.layers.29.self_attn.v_proj.weight: torch.int8
 - model.layers.29.self_attn.o_proj.weight: torch.int8
 - model.layers.29.mlp.gate_proj.weight: torch.int8
 - model.layers.29.mlp.up_proj.weight: torch.int8
 - model.layers.29.mlp.down_proj.weight: torch.int8
 - model.layers.29.input_layernorm.weight: torch.float16
 - model.layers.29.post_attention_layernorm.weight: torch.float16
 - model.layers.30.self_attn.q_proj.weight: torch.int8
 - model.layers.30.self_attn.k_proj.weight: torch.int8
 - model.layers.30.self_attn.v_proj.weight: torch.int8
 - model.layers.30.self_attn.o_proj.weight: torch.int8
 - model.layers.30.mlp.gate_proj.weight: torch.int8
 - model.layers.30.mlp.up_proj.weight: torch.int8
 - model.layers.30.mlp.down_proj.weight: torch.int8
 - model.layers.30.input_layernorm.weight: torch.float16
 - model.layers.30.post_attention_layernorm.weight: torch.float16
 - model.layers.31.self_attn.q_proj.weight: torch.int8
 - model.layers.31.self_attn.k_proj.weight: torch.int8
 - model.layers.31.self_attn.v_proj.weight: torch.int8
 - model.layers.31.self_attn.o_proj.weight: torch.int8
 - model.layers.31.mlp.gate_proj.weight: torch.int8
 - model.layers.31.mlp.up_proj.weight: torch.int8
 - model.layers.31.mlp.down_proj.weight: torch.int8
 - model.layers.31.input_layernorm.weight: torch.float16
 - model.layers.31.post_attention_layernorm.weight: torch.float16
 - model.layers.32.self_attn.q_proj.weight: torch.int8
 - model.layers.32.self_attn.k_proj.weight: torch.int8
 - model.layers.32.self_attn.v_proj.weight: torch.int8
 - model.layers.32.self_attn.o_proj.weight: torch.int8
 - model.layers.32.mlp.gate_proj.weight: torch.int8
 - model.layers.32.mlp.up_proj.weight: torch.int8
 - model.layers.32.mlp.down_proj.weight: torch.int8
 - model.layers.32.input_layernorm.weight: torch.float16
 - model.layers.32.post_attention_layernorm.weight: torch.float16
 - model.layers.33.self_attn.q_proj.weight: torch.int8
 - model.layers.33.self_attn.k_proj.weight: torch.int8
 - model.layers.33.self_attn.v_proj.weight: torch.int8
 - model.layers.33.self_attn.o_proj.weight: torch.int8
 - model.layers.33.mlp.gate_proj.weight: torch.int8
 - model.layers.33.mlp.up_proj.weight: torch.int8
 - model.layers.33.mlp.down_proj.weight: torch.int8
 - model.layers.33.input_layernorm.weight: torch.float16
 - model.layers.33.post_attention_layernorm.weight: torch.float16
 - model.layers.34.self_attn.q_proj.weight: torch.int8
 - model.layers.34.self_attn.k_proj.weight: torch.int8
 - model.layers.34.self_attn.v_proj.weight: torch.int8
 - model.layers.34.self_attn.o_proj.weight: torch.int8
 - model.layers.34.mlp.gate_proj.weight: torch.int8
 - model.layers.34.mlp.up_proj.weight: torch.int8
 - model.layers.34.mlp.down_proj.weight: torch.int8
 - model.layers.34.input_layernorm.weight: torch.float16
 - model.layers.34.post_attention_layernorm.weight: torch.float16
 - model.layers.35.self_attn.q_proj.weight: torch.int8
 - model.layers.35.self_attn.k_proj.weight: torch.int8
 - model.layers.35.self_attn.v_proj.weight: torch.int8
 - model.layers.35.self_attn.o_proj.weight: torch.int8
 - model.layers.35.mlp.gate_proj.weight: torch.int8
 - model.layers.35.mlp.up_proj.weight: torch.int8
 - model.layers.35.mlp.down_proj.weight: torch.int8
 - model.layers.35.input_layernorm.weight: torch.float16
 - model.layers.35.post_attention_layernorm.weight: torch.float16
 - model.layers.36.self_attn.q_proj.weight: torch.int8
 - model.layers.36.self_attn.k_proj.weight: torch.int8
 - model.layers.36.self_attn.v_proj.weight: torch.int8
 - model.layers.36.self_attn.o_proj.weight: torch.int8
 - model.layers.36.mlp.gate_proj.weight: torch.int8
 - model.layers.36.mlp.up_proj.weight: torch.int8
 - model.layers.36.mlp.down_proj.weight: torch.int8
 - model.layers.36.input_layernorm.weight: torch.float16
 - model.layers.36.post_attention_layernorm.weight: torch.float16
 - model.layers.37.self_attn.q_proj.weight: torch.int8
 - model.layers.37.self_attn.k_proj.weight: torch.int8
 - model.layers.37.self_attn.v_proj.weight: torch.int8
 - model.layers.37.self_attn.o_proj.weight: torch.int8
 - model.layers.37.mlp.gate_proj.weight: torch.int8
 - model.layers.37.mlp.up_proj.weight: torch.int8
 - model.layers.37.mlp.down_proj.weight: torch.int8
 - model.layers.37.input_layernorm.weight: torch.float16
 - model.layers.37.post_attention_layernorm.weight: torch.float16
 - model.layers.38.self_attn.q_proj.weight: torch.int8
 - model.layers.38.self_attn.k_proj.weight: torch.int8
 - model.layers.38.self_attn.v_proj.weight: torch.int8
 - model.layers.38.self_attn.o_proj.weight: torch.int8
 - model.layers.38.mlp.gate_proj.weight: torch.int8
 - model.layers.38.mlp.up_proj.weight: torch.int8
 - model.layers.38.mlp.down_proj.weight: torch.int8
 - model.layers.38.input_layernorm.weight: torch.float16
 - model.layers.38.post_attention_layernorm.weight: torch.float16
 - model.layers.39.self_attn.q_proj.weight: torch.int8
 - model.layers.39.self_attn.k_proj.weight: torch.int8
 - model.layers.39.self_attn.v_proj.weight: torch.int8
 - model.layers.39.self_attn.o_proj.weight: torch.int8
 - model.layers.39.mlp.gate_proj.weight: torch.int8
 - model.layers.39.mlp.up_proj.weight: torch.int8
 - model.layers.39.mlp.down_proj.weight: torch.int8
 - model.layers.39.input_layernorm.weight: torch.float16
 - model.layers.39.post_attention_layernorm.weight: torch.float16
 - model.layers.40.self_attn.q_proj.weight: torch.int8
 - model.layers.40.self_attn.k_proj.weight: torch.int8
 - model.layers.40.self_attn.v_proj.weight: torch.int8
 - model.layers.40.self_attn.o_proj.weight: torch.int8
 - model.layers.40.mlp.gate_proj.weight: torch.int8
 - model.layers.40.mlp.up_proj.weight: torch.int8
 - model.layers.40.mlp.down_proj.weight: torch.int8
 - model.layers.40.input_layernorm.weight: torch.float16
 - model.layers.40.post_attention_layernorm.weight: torch.float16
 - model.layers.41.self_attn.q_proj.weight: torch.int8
 - model.layers.41.self_attn.k_proj.weight: torch.int8
 - model.layers.41.self_attn.v_proj.weight: torch.int8
 - model.layers.41.self_attn.o_proj.weight: torch.int8
 - model.layers.41.mlp.gate_proj.weight: torch.int8
 - model.layers.41.mlp.up_proj.weight: torch.int8
 - model.layers.41.mlp.down_proj.weight: torch.int8
 - model.layers.41.input_layernorm.weight: torch.float16
 - model.layers.41.post_attention_layernorm.weight: torch.float16
 - model.layers.42.self_attn.q_proj.weight: torch.int8
 - model.layers.42.self_attn.k_proj.weight: torch.int8
 - model.layers.42.self_attn.v_proj.weight: torch.int8
 - model.layers.42.self_attn.o_proj.weight: torch.int8
 - model.layers.42.mlp.gate_proj.weight: torch.int8
 - model.layers.42.mlp.up_proj.weight: torch.int8
 - model.layers.42.mlp.down_proj.weight: torch.int8
 - model.layers.42.input_layernorm.weight: torch.float16
 - model.layers.42.post_attention_layernorm.weight: torch.float16
 - model.layers.43.self_attn.q_proj.weight: torch.int8
 - model.layers.43.self_attn.k_proj.weight: torch.int8
 - model.layers.43.self_attn.v_proj.weight: torch.int8
 - model.layers.43.self_attn.o_proj.weight: torch.int8
 - model.layers.43.mlp.gate_proj.weight: torch.int8
 - model.layers.43.mlp.up_proj.weight: torch.int8
 - model.layers.43.mlp.down_proj.weight: torch.int8
 - model.layers.43.input_layernorm.weight: torch.float16
 - model.layers.43.post_attention_layernorm.weight: torch.float16
 - model.layers.44.self_attn.q_proj.weight: torch.int8
 - model.layers.44.self_attn.k_proj.weight: torch.int8
 - model.layers.44.self_attn.v_proj.weight: torch.int8
 - model.layers.44.self_attn.o_proj.weight: torch.int8
 - model.layers.44.mlp.gate_proj.weight: torch.int8
 - model.layers.44.mlp.up_proj.weight: torch.int8
 - model.layers.44.mlp.down_proj.weight: torch.int8
 - model.layers.44.input_layernorm.weight: torch.float16
 - model.layers.44.post_attention_layernorm.weight: torch.float16
 - model.layers.45.self_attn.q_proj.weight: torch.int8
 - model.layers.45.self_attn.k_proj.weight: torch.int8
 - model.layers.45.self_attn.v_proj.weight: torch.int8
 - model.layers.45.self_attn.o_proj.weight: torch.int8
 - model.layers.45.mlp.gate_proj.weight: torch.int8
 - model.layers.45.mlp.up_proj.weight: torch.int8
 - model.layers.45.mlp.down_proj.weight: torch.int8
 - model.layers.45.input_layernorm.weight: torch.float16
 - model.layers.45.post_attention_layernorm.weight: torch.float16
 - model.layers.46.self_attn.q_proj.weight: torch.int8
 - model.layers.46.self_attn.k_proj.weight: torch.int8
 - model.layers.46.self_attn.v_proj.weight: torch.int8
 - model.layers.46.self_attn.o_proj.weight: torch.int8
 - model.layers.46.mlp.gate_proj.weight: torch.int8
 - model.layers.46.mlp.up_proj.weight: torch.int8
 - model.layers.46.mlp.down_proj.weight: torch.int8
 - model.layers.46.input_layernorm.weight: torch.float16
 - model.layers.46.post_attention_layernorm.weight: torch.float16
 - model.layers.47.self_attn.q_proj.weight: torch.int8
 - model.layers.47.self_attn.k_proj.weight: torch.int8
 - model.layers.47.self_attn.v_proj.weight: torch.int8
 - model.layers.47.self_attn.o_proj.weight: torch.int8
 - model.layers.47.mlp.gate_proj.weight: torch.int8
 - model.layers.47.mlp.up_proj.weight: torch.int8
 - model.layers.47.mlp.down_proj.weight: torch.int8
 - model.layers.47.input_layernorm.weight: torch.float16
 - model.layers.47.post_attention_layernorm.weight: torch.float16
 - model.layers.48.self_attn.q_proj.weight: torch.int8
 - model.layers.48.self_attn.k_proj.weight: torch.int8
 - model.layers.48.self_attn.v_proj.weight: torch.int8
 - model.layers.48.self_attn.o_proj.weight: torch.int8
 - model.layers.48.mlp.gate_proj.weight: torch.int8
 - model.layers.48.mlp.up_proj.weight: torch.int8
 - model.layers.48.mlp.down_proj.weight: torch.int8
 - model.layers.48.input_layernorm.weight: torch.float16
 - model.layers.48.post_attention_layernorm.weight: torch.float16
 - model.layers.49.self_attn.q_proj.weight: torch.int8
 - model.layers.49.self_attn.k_proj.weight: torch.int8
 - model.layers.49.self_attn.v_proj.weight: torch.int8
 - model.layers.49.self_attn.o_proj.weight: torch.int8
 - model.layers.49.mlp.gate_proj.weight: torch.int8
 - model.layers.49.mlp.up_proj.weight: torch.int8
 - model.layers.49.mlp.down_proj.weight: torch.int8
 - model.layers.49.input_layernorm.weight: torch.float16
 - model.layers.49.post_attention_layernorm.weight: torch.float16
 - model.layers.50.self_attn.q_proj.weight: torch.int8
 - model.layers.50.self_attn.k_proj.weight: torch.int8
 - model.layers.50.self_attn.v_proj.weight: torch.int8
 - model.layers.50.self_attn.o_proj.weight: torch.int8
 - model.layers.50.mlp.gate_proj.weight: torch.int8
 - model.layers.50.mlp.up_proj.weight: torch.int8
 - model.layers.50.mlp.down_proj.weight: torch.int8
 - model.layers.50.input_layernorm.weight: torch.float16
 - model.layers.50.post_attention_layernorm.weight: torch.float16
 - model.layers.51.self_attn.q_proj.weight: torch.int8
 - model.layers.51.self_attn.k_proj.weight: torch.int8
 - model.layers.51.self_attn.v_proj.weight: torch.int8
 - model.layers.51.self_attn.o_proj.weight: torch.int8
 - model.layers.51.mlp.gate_proj.weight: torch.int8
 - model.layers.51.mlp.up_proj.weight: torch.int8
 - model.layers.51.mlp.down_proj.weight: torch.int8
 - model.layers.51.input_layernorm.weight: torch.float16
 - model.layers.51.post_attention_layernorm.weight: torch.float16
 - model.layers.52.self_attn.q_proj.weight: torch.int8
 - model.layers.52.self_attn.k_proj.weight: torch.int8
 - model.layers.52.self_attn.v_proj.weight: torch.int8
 - model.layers.52.self_attn.o_proj.weight: torch.int8
 - model.layers.52.mlp.gate_proj.weight: torch.int8
 - model.layers.52.mlp.up_proj.weight: torch.int8
 - model.layers.52.mlp.down_proj.weight: torch.int8
 - model.layers.52.input_layernorm.weight: torch.float16
 - model.layers.52.post_attention_layernorm.weight: torch.float16
 - model.layers.53.self_attn.q_proj.weight: torch.int8
 - model.layers.53.self_attn.k_proj.weight: torch.int8
 - model.layers.53.self_attn.v_proj.weight: torch.int8
 - model.layers.53.self_attn.o_proj.weight: torch.int8
 - model.layers.53.mlp.gate_proj.weight: torch.int8
 - model.layers.53.mlp.up_proj.weight: torch.int8
 - model.layers.53.mlp.down_proj.weight: torch.int8
 - model.layers.53.input_layernorm.weight: torch.float16
 - model.layers.53.post_attention_layernorm.weight: torch.float16
 - model.layers.54.self_attn.q_proj.weight: torch.int8
 - model.layers.54.self_attn.k_proj.weight: torch.int8
 - model.layers.54.self_attn.v_proj.weight: torch.int8
 - model.layers.54.self_attn.o_proj.weight: torch.int8
 - model.layers.54.mlp.gate_proj.weight: torch.int8
 - model.layers.54.mlp.up_proj.weight: torch.int8
 - model.layers.54.mlp.down_proj.weight: torch.int8
 - model.layers.54.input_layernorm.weight: torch.float16
 - model.layers.54.post_attention_layernorm.weight: torch.float16
 - model.layers.55.self_attn.q_proj.weight: torch.int8
 - model.layers.55.self_attn.k_proj.weight: torch.int8
 - model.layers.55.self_attn.v_proj.weight: torch.int8
 - model.layers.55.self_attn.o_proj.weight: torch.int8
 - model.layers.55.mlp.gate_proj.weight: torch.int8
 - model.layers.55.mlp.up_proj.weight: torch.int8
 - model.layers.55.mlp.down_proj.weight: torch.int8
 - model.layers.55.input_layernorm.weight: torch.float16
 - model.layers.55.post_attention_layernorm.weight: torch.float16
 - model.layers.56.self_attn.q_proj.weight: torch.int8
 - model.layers.56.self_attn.k_proj.weight: torch.int8
 - model.layers.56.self_attn.v_proj.weight: torch.int8
 - model.layers.56.self_attn.o_proj.weight: torch.int8
 - model.layers.56.mlp.gate_proj.weight: torch.int8
 - model.layers.56.mlp.up_proj.weight: torch.int8
 - model.layers.56.mlp.down_proj.weight: torch.int8
 - model.layers.56.input_layernorm.weight: torch.float16
 - model.layers.56.post_attention_layernorm.weight: torch.float16
 - model.layers.57.self_attn.q_proj.weight: torch.int8
 - model.layers.57.self_attn.k_proj.weight: torch.int8
 - model.layers.57.self_attn.v_proj.weight: torch.int8
 - model.layers.57.self_attn.o_proj.weight: torch.int8
 - model.layers.57.mlp.gate_proj.weight: torch.int8
 - model.layers.57.mlp.up_proj.weight: torch.int8
 - model.layers.57.mlp.down_proj.weight: torch.int8
 - model.layers.57.input_layernorm.weight: torch.float16
 - model.layers.57.post_attention_layernorm.weight: torch.float16
 - model.layers.58.self_attn.q_proj.weight: torch.int8
 - model.layers.58.self_attn.k_proj.weight: torch.int8
 - model.layers.58.self_attn.v_proj.weight: torch.int8
 - model.layers.58.self_attn.o_proj.weight: torch.int8
 - model.layers.58.mlp.gate_proj.weight: torch.int8
 - model.layers.58.mlp.up_proj.weight: torch.int8
 - model.layers.58.mlp.down_proj.weight: torch.int8
 - model.layers.58.input_layernorm.weight: torch.float16
 - model.layers.58.post_attention_layernorm.weight: torch.float16
 - model.layers.59.self_attn.q_proj.weight: torch.int8
 - model.layers.59.self_attn.k_proj.weight: torch.int8
 - model.layers.59.self_attn.v_proj.weight: torch.int8
 - model.layers.59.self_attn.o_proj.weight: torch.int8
 - model.layers.59.mlp.gate_proj.weight: torch.int8
 - model.layers.59.mlp.up_proj.weight: torch.int8
 - model.layers.59.mlp.down_proj.weight: torch.int8
 - model.layers.59.input_layernorm.weight: torch.float16
 - model.layers.59.post_attention_layernorm.weight: torch.float16
 - model.layers.60.self_attn.q_proj.weight: torch.int8
 - model.layers.60.self_attn.k_proj.weight: torch.int8
 - model.layers.60.self_attn.v_proj.weight: torch.int8
 - model.layers.60.self_attn.o_proj.weight: torch.int8
 - model.layers.60.mlp.gate_proj.weight: torch.int8
 - model.layers.60.mlp.up_proj.weight: torch.int8
 - model.layers.60.mlp.down_proj.weight: torch.int8
 - model.layers.60.input_layernorm.weight: torch.float16
 - model.layers.60.post_attention_layernorm.weight: torch.float16
 - model.layers.61.self_attn.q_proj.weight: torch.int8
 - model.layers.61.self_attn.k_proj.weight: torch.int8
 - model.layers.61.self_attn.v_proj.weight: torch.int8
 - model.layers.61.self_attn.o_proj.weight: torch.int8
 - model.layers.61.mlp.gate_proj.weight: torch.int8
 - model.layers.61.mlp.up_proj.weight: torch.int8
 - model.layers.61.mlp.down_proj.weight: torch.int8
 - model.layers.61.input_layernorm.weight: torch.float16
 - model.layers.61.post_attention_layernorm.weight: torch.float16
 - model.layers.62.self_attn.q_proj.weight: torch.int8
 - model.layers.62.self_attn.k_proj.weight: torch.int8
 - model.layers.62.self_attn.v_proj.weight: torch.int8
 - model.layers.62.self_attn.o_proj.weight: torch.int8
 - model.layers.62.mlp.gate_proj.weight: torch.int8
 - model.layers.62.mlp.up_proj.weight: torch.int8
 - model.layers.62.mlp.down_proj.weight: torch.int8
 - model.layers.62.input_layernorm.weight: torch.float16
 - model.layers.62.post_attention_layernorm.weight: torch.float16
 - model.layers.63.self_attn.q_proj.weight: torch.int8
 - model.layers.63.self_attn.k_proj.weight: torch.int8
 - model.layers.63.self_attn.v_proj.weight: torch.int8
 - model.layers.63.self_attn.o_proj.weight: torch.int8
 - model.layers.63.mlp.gate_proj.weight: torch.int8
 - model.layers.63.mlp.up_proj.weight: torch.int8
 - model.layers.63.mlp.down_proj.weight: torch.int8
 - model.layers.63.input_layernorm.weight: torch.float16
 - model.layers.63.post_attention_layernorm.weight: torch.float16
 - model.norm.weight: torch.float16
 - lm_head.weight: torch.float16

{
  "results": {
    "kobest_hellaswag": {
      "acc": 0.55,
      "acc_stderr": 0.02227087748536047,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.02193084412072858,
      "macro_f1": 0.5479369940908042,
      "macro_f1_stderr": 0.0223050665693464
    }
  },
  "versions": {
    "kobest_hellaswag": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=x2bee/POLAR-14B-v0.2,use_accelerate=true,trust_remote_code=true",
    "num_fewshot": 0,
    "batch_size": "8",
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "quantization_module": "bnb",
    "bit_width": "int8"
  }
}
hf-causal-experimental (pretrained=x2bee/POLAR-14B-v0.2,use_accelerate=true,trust_remote_code=true), limit: None, provide_description: False, num_fewshot: 0, batch_size: 8
|      Task      |Version| Metric |Value |   |Stderr|
|----------------|------:|--------|-----:|---|-----:|
|kobest_hellaswag|      0|acc     |0.5500|±  |0.0223|
|                |       |acc_norm|0.6000|±  |0.0219|
|                |       |macro_f1|0.5479|±  |0.0223|

